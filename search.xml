<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python爬虫实战之大众点评]]></title>
    <url>%2F2019%2F02%2F26%2FPython%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%E4%B9%8B%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;大二上学期的综合程序设计，挑了个应该最水的爬虫（懒得花功夫在这个水课上Orz），结果最后一直从开学咕咕咕到期末，考试周两天抽空把才敲完了（严重导致编译技术没复习好o(╥﹏╥)o），在队友也是鸽子的情况下，拖到最后果然不是个好习惯2333。好吧废话少说，开始正题简短地记录下。 网站分析&emsp;&emsp;这次我们选取的是Web端的大众点评，准备爬取的信息是商铺的信息，首先爬取区域链接，再分区域根据其智能排序爬取他的店名，id，链接，星级。 &emsp;&emsp;本身网页的分析并不困难，比较麻烦的点是大众点评的反爬机制有点麻烦，需要综合下各种方法。然后就是用多进程对爬虫的性能进行了下提升，最后还临时补了一个登陆模块。 &emsp;&emsp;之后分析我们要爬取的数据时静态的，没有经过js渲染所以会简单一点（如果有动态渲染的话，应该就是利用selenium来进行模拟好做），直接用requests+Beautifulsoup进行网页的爬取和分析，我们观察它的前端页面： &emsp;&emsp;观察他的id=”bussi-nav”，然后在这个div下遍历下他的下级标签，正则匹配出链接href，存储在一个list中并返回. 123456789101112131415def getRegion():#返回要爬取的区域的链接,返回一个数组 href=[]#用来存储每个区域的链接 url=&quot;http://www.dianping.com/chengdu/ch10/r1579&quot; try: soup=UseBeautifulSoup(url) s=soup.find_all(id=&quot;bussi-nav&quot;)[0]#注意到区域的div的id是bussi-nav,find_all方法筛选到 for m in s.select(&apos;a&apos;): href.append(re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(m))[0])#正则找到链接，存储到href数组中,注意这里re.fnidall返回的是一个数组,取第一项 print(&apos;爬取地区链接成功&apos;) return href except: print(&apos;爬取地区链接失败&apos;) print(soup) os._exit() &emsp;&emsp;之后看店铺的: &emsp;&emsp;观察它的a标签中data-click-name=”shop_title_click”但是这里不能直接用这个tag属性进行搜索，需要用attrs方法转换为键值对，然后遍历这个标签下每个店铺的名称 ，链接，并从链接中取出唯一id存放在dict中，然后看星级的class=’comment’，同样遍历存储在一个dict中，最后再与前面的信息进行匹配，放在一个字典中。 1234567891011121314151617181920212223242526272829@retry(retry_on_result=retry_if_result_none)#当出现错误（没有爬取到star）时，进行异常重试def getValue(url):#获取当面的店名以及他的id，链接，星级，返回一个字典 name_And_value=&#123;&#125; value=&#123;&apos;id&apos;:&apos;&apos;,&apos;href&apos;:&apos;&apos;,&apos;star&apos;:&apos;&apos;&#125; count=0 try: soup=UseBeautifulSoup(url) star=get_Star(soup)#获得这面的star if len(star)==0:#没有爬到，返回个None return None shoplist=soup.find_all(&apos;a&apos;,attrs=&#123;&quot;data-click-name&quot;:&quot;shop_title_click&quot;&#125;)#注意这里有些tag属性是不能用来搜索的，例如HTML5中的data-*属性，我们需要用attrs方法 for i in shoplist: temp=value.copy()#这里我遇到一个坑点，在字典1中添加字典2其指向的应该是被添加字典2的地址，并没有新建一个对象，所以在循环汇总改变字典2的值，有点类似深浅拷贝，也会改变字典1里面的值具体参考：https://blog.csdn.net/sinat_21302587/article/details/72356431 name=re.findall(r&quot;title=\&quot;(.+?)\&quot;&quot;,str(i))[0]#正则寻找店名及 temp[&apos;href&apos;]=re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(i))[0]#链接存储 temp[&apos;id&apos;]=re.findall(r&quot;http://www.dianping.com/shop/(.+)&quot;,str(temp[&apos;href&apos;]))[0]#获取每个店铺的id值 temp[&apos;star&apos;]=star[count]#存入对应的星级 name_And_value[name]=temp count+=1 return name_And_value except: print(&apos;爬取区域店铺失败&apos;) print(soup) os._exit() return 1 &emsp;&emsp;这个基本上就是爬取信息的最基本流程，但是这仅仅是个开始Orz，最重要的还是怎么绕过他的反爬机制。 反爬虫分析&emsp;&emsp;经过我试了无数次的结果，大概上猜测出大众点评的反爬逻辑，大概有这几点： 一定要带上headers，否则直接报403 需要不断更改user-agent,referer，显著提高成功率，尤其是不断更换referer后效果明显 根据ip的访问频率，否则会200报一个访问页面不存在（但是使用代理ip后效果不是特别明显，怀疑主要是根据cookie） 根据cookie的访问频率（八成把握），效果同上 某个cookie或者ip访问达到一定次数（频率）会跳验证码，并且还是双重的，第一个是个拖动，第二个是个图像 不同的页面反爬虫的程度不同，商家店铺页面会更严格（猜测的，在爬取测试的时候，但是也有可能跟爬取的量有关，还没有达到他的阈值） &emsp;&emsp;所以针对以上问题，就需要构建一个ua池，构建一个referer池，本来还应该构建一个ip池，但是那些免费的代理网站的ip实在不稳定，筛选下来的成功率太低了，就直接使用了付费ip代理，这里我选择是阿布云代理，一块钱一小时，因为这次的爬虫不是很大所以也可以接受（下次试试自己搭建ip池），其实在这里爬虫的成功率就已经很高了估计有个七八成，基本上达到我们的要求，所以就没有搭建cookie池（可能搭建个cookie池会更好） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def get_proxy():#使用阿布云的代理接口 # 要访问的目标页面 targetUrl = &quot;http://test.abuyun.com&quot; # 代理服务器 proxyHost = &quot;http-dyn.abuyun.com&quot; proxyPort = &quot;9020&quot; # 代理隧道验证信息 proxyUser = &quot;HS9039NV974Y34KD&quot; proxyPass = &quot;B5687CE5E8CFD090&quot; proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123; &quot;host&quot; : proxyHost, &quot;port&quot; : proxyPort, &quot;user&quot; : proxyUser, &quot;pass&quot; : proxyPass, &#125; proxies = &#123; &quot;http&quot; : proxyMeta, &quot;https&quot; : proxyMeta, &#125; return proxiesdef get_ua():#构建ua池 user_agents = [ &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&apos;, &apos;Opera/8.0 (Windows NT 5.1; U; en)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&apos;, &apos;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2 &apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&apos;, &apos;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&apos;, &apos;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0&apos;, &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0) &apos;, ] user_agent = random.choice(user_agents) #random.choice(),从列表中随机抽取一个对象 return user_agentdef get_ref():#构建referer池 referers=[&apos;http://www.dianping.com/chengdu/ch10/r7949&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1577&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7767&apos;, &apos;http://www.dianping.com/chengdu/ch10/r5894&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1592&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1601&apos;, &apos;http://www.dianping.com/chengdu/ch10/r70146&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7764&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7771&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7769&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1597&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7768&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1974&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1578&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1604&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1579&apos; ] referer=random.choice(referers) return referer@retry#利用retry装饰器，异常重试def UseBeautifulSoup(url):#使用beautifulsoup处理 ua=get_ua()#获得随机的User-Agent referer=get_ref()#获得随机的Referer proxy=get_proxy()#获得代理 with open(&apos;D:/cookie.ini&apos;, &apos;rb&apos;) as f:#读取cookie data = pickle.load(f) for cookie in data: cookies=&#123;cookie[&apos;name&apos;]:cookie[&apos;value&apos;]&#125; headers=&#123; &quot;User-Agent&quot;:ua, &quot;Referer&quot;:referer, &quot;Cookie&quot;:str(cookies),#这里需要str()下 &#125;#这里需要加个header，否则直接给返回个403 res=requests.get(url,headers=headers,proxies=proxy)#使用代理 res.encoding=&apos;utf-8&apos; soup=BeautifulSoup(res.text,&apos;html.parser&apos;) return soup 代码优化&emsp;&emsp;之后就是对爬虫一点的优化了，这里我选择使用了多进程来进行爬取（Python的多线程虽然是伪多线程，但是爬虫的话也可以使用，因为爬虫是一个I/O密集型的，不是计算密集型的），然后由于代理ip接口可能不稳定啊，或者爬取失败，这里使用了retry模块，利用retry装饰器进行异常重试，最后用pickle序列化存储数据，之后数据交给队友存到数据库里（总要象征性的让他们有点参与感233）。 &emsp;&emsp;retry装饰器在前面一段代码，一下是使用多进程对爬取不同页面： 123456789101112131415161718192021222324252627282930313233343536def conbineResult(t):#将每次爬取的结果写入文件中，多进程并发写入同一文件时会出现资源竞争，所以使用Pool类中的callback方法调用回调函数 with open(r&apos;D:/outp.txt&apos;,&apos;ab&apos;) as f:#注意pickle默认使用默认使用二进制存储，这里不能用a if t!=1: pickle.dump(t,f)#序列化存储结果 def main(): href=getRegion()#获得各个区域的链接 pool=multiprocessing.Pool(processes=4)#建立进程池,默认为cpu核数 print(href) print(&apos;区域数量为&apos;+str(len(href))) for i in range(len(href)): for index in range(1,3):#取1到5面的内容 if index==1: realurl=href[i] else: realurl=href[i]+&apos;p&apos;+str(index)#注意到每面的链接就是在第一面的基础上加上p+页数 print(realurl) result=pool.apply_async(getValue,(realurl,),callback=conbineResult)#添加进程，并且调用回调函数conbineResult处理结果 pool.close()#关闭进程池，不再添加进程 pool.join()#因为是非阻塞的，主进程继续执行直到遇到.join()等待pool中所有子进程完毕，主进程才继续 if result.successful(): print(&apos;successful&apos;) &apos;&apos;&apos; with open(r&apos;D:/outp.txt&apos;,&apos;rb&apos;) as f:#对于多次dump,pickle每次序列化生成的字符串有独立头尾,pickle.load()只会读取一个完整的结果,需要循环读出 try: while True: datas=pickle.load(f)#取出数据 print(datas) except: print(&apos;success&apos;) &apos;&apos;&apos;if __name__ == &quot;__main__&quot;: main() 登录模块&emsp;&emsp;最后发现综设报告上还有个要求，需要加个登录模块，想想直接用selenium方便简单，也不需要分析包了。这里有个坑，在登录页面，密码登录界面需要切换下，它是内嵌了个iframe。 123456789101112131415161718192021222324252627282930313233from selenium import webdriverimport requestsimport timeimport pickledef main(): chromePath=r&apos;C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe&apos;#chromedriver的路径 wd = webdriver.Chrome(executable_path= chromePath)#构建浏览器 url=r&apos;https://account.dianping.com/login?redir=http%3A%2F%2Fwww.dianping.com%2F&apos; wd.get(url) iframe = wd.find_element_by_xpath(&apos;//*[@id=&quot;J_login_container&quot;]/div/iframe&apos;)#切换到登陆模块 wd.switch_to_frame(iframe) icon_pc = wd.find_element_by_xpath(&apos;/html/body/div/div[2]/div[5]/span&apos;)#选择账户登录 icon_pc.click() time.sleep(2) name_login = wd.find_element_by_xpath(&apos;//*[@id=&quot;tab-account&quot;]&apos;)#选择手机密码登录登录 name_login.click() time.sleep(2) wd.find_element_by_id(&apos;account-textbox&apos;).send_keys(&apos;**********&apos;)#输入手机号 wd.find_element_by_xpath(&apos;//input[@id=&quot;password-textbox&quot;]&apos;).send_keys(&apos;*******&apos;)#输入密码 wd.find_element_by_xpath(&apos;//button[@id=&quot;login-button-account&quot;]&apos;).click()#点击登录 cookies=wd.get_cookies()#cookies存储了登录后的cookie print(cookies) time.sleep(5) wd.quit()#退出浏览器 with open(&apos;D:/cookie.ini&apos;,&apos;wb&apos;) as f:#使用pickle序列化存储cookie pickle.dump(cookies,f)if __name__==&apos;__main__&apos;: main() &emsp;&emsp;最后将登录过后的cookie保存下来作为之后的请求cookie 总结&emsp;&emsp;主要的时间花在了如何绕过反爬虫，顺便了解了下Python的多线程，多进程，这次做的也还是很粗糙，有很多地方可以优化，大概对爬虫还有几个坑要填 多线程 验证码识别 scrapy框架&emsp;&emsp;最后贴上总的代码，登录模块代码前面已贴过，这里只放主程序。使用时要先跑下登录模块获得cookie再跑主程序。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188# -*- coding: UTF-8 -*-import requestsimport reimport randomfrom bs4 import BeautifulSoupfrom retrying import retryimport osimport multiprocessingimport pickledef get_proxy():#使用阿布云的代理接口 # 要访问的目标页面 targetUrl = &quot;http://test.abuyun.com&quot; # 代理服务器 proxyHost = &quot;http-dyn.abuyun.com&quot; proxyPort = &quot;9020&quot; # 代理隧道验证信息 proxyUser = &quot;HS9039NV974Y34KD&quot; proxyPass = &quot;B5687CE5E8CFD090&quot; proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123; &quot;host&quot; : proxyHost, &quot;port&quot; : proxyPort, &quot;user&quot; : proxyUser, &quot;pass&quot; : proxyPass, &#125; proxies = &#123; &quot;http&quot; : proxyMeta, &quot;https&quot; : proxyMeta, &#125; return proxiesdef get_ua():#构建ua池 user_agents = [ &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&apos;, &apos;Opera/8.0 (Windows NT 5.1; U; en)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&apos;, &apos;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2 &apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&apos;, &apos;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&apos;, &apos;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0&apos;, &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0) &apos;, ] user_agent = random.choice(user_agents) #random.choice(),从列表中随机抽取一个对象 return user_agentdef get_ref():#构建referer池 referers=[&apos;http://www.dianping.com/chengdu/ch10/r7949&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1577&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7767&apos;, &apos;http://www.dianping.com/chengdu/ch10/r5894&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1592&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1601&apos;, &apos;http://www.dianping.com/chengdu/ch10/r70146&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7764&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7771&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7769&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1597&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7768&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1974&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1578&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1604&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1579&apos; ] referer=random.choice(referers) return referer@retry#利用retry装饰器，异常重试def UseBeautifulSoup(url):#使用beautifulsoup处理 ua=get_ua()#获得随机的User-Agent referer=get_ref()#获得随机的Referer proxy=get_proxy()#获得代理 with open(&apos;D:/cookie.ini&apos;, &apos;rb&apos;) as f:#读取cookie data = pickle.load(f) for cookie in data: cookies=&#123;cookie[&apos;name&apos;]:cookie[&apos;value&apos;]&#125; headers=&#123; &quot;User-Agent&quot;:ua, &quot;Referer&quot;:referer, &quot;Cookie&quot;:str(cookies),#这里需要str()下 &#125;#这里需要加个header，否则直接给返回个403 res=requests.get(url,headers=headers,proxies=proxy)#使用代理 res.encoding=&apos;utf-8&apos; soup=BeautifulSoup(res.text,&apos;html.parser&apos;) return soup def getRegion():#返回要爬取的区域的链接,返回一个数组 href=[]#用来存储每个区域的链接 url=&quot;http://www.dianping.com/chengdu/ch10/r1579&quot; try: soup=UseBeautifulSoup(url) s=soup.find_all(id=&quot;bussi-nav&quot;)[0]#注意到区域的div的id是bussi-nav,find_all方法筛选到 for m in s.select(&apos;a&apos;): href.append(re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(m))[0])#正则找到链接，存储到href数组中,注意这里re.fnidall返回的是一个数组,取第一项 print(&apos;爬取地区链接成功&apos;) return href except: print(&apos;爬取地区链接失败&apos;) print(soup) os._exit()def retry_if_result_none(result): return result is None@retry(retry_on_result=retry_if_result_none)#当出现错误（没有爬取到star）时，进行异常重试def getValue(url):#获取当面的店名以及他的id，链接，星级，返回一个字典 name_And_value=&#123;&#125; value=&#123;&apos;id&apos;:&apos;&apos;,&apos;href&apos;:&apos;&apos;,&apos;star&apos;:&apos;&apos;&#125; count=0 try: soup=UseBeautifulSoup(url) star=get_Star(soup)#获得这面的star if len(star)==0:#没有爬到，返回个None return None shoplist=soup.find_all(&apos;a&apos;,attrs=&#123;&quot;data-click-name&quot;:&quot;shop_title_click&quot;&#125;)#注意这里有些tag属性是不能用来搜索的，例如HTML5中的data-*属性，我们需要用attrs方法 for i in shoplist: temp=value.copy()#这里我遇到一个坑点，在字典1中添加字典2其指向的应该是被添加字典2的地址，并没有新建一个对象，所以在循环汇总改变字典2的值，有点类似深浅拷贝，也会改变字典1里面的值具体参考：https://blog.csdn.net/sinat_21302587/article/details/72356431 name=re.findall(r&quot;title=\&quot;(.+?)\&quot;&quot;,str(i))[0]#正则寻找店名及 temp[&apos;href&apos;]=re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(i))[0]#链接存储 temp[&apos;id&apos;]=re.findall(r&quot;http://www.dianping.com/shop/(.+)&quot;,str(temp[&apos;href&apos;]))[0]#获取每个店铺的id值 temp[&apos;star&apos;]=star[count]#存入对应的星级 name_And_value[name]=temp count+=1 return name_And_value except: print(&apos;爬取区域店铺失败&apos;) print(soup) os._exit() return 1def get_Star(soup):#获取商户的星级 star=[] comment=soup.find_all(&apos;div&apos;,class_=&apos;comment&apos;)#先找到comment标签 for i in comment: star.append(re.findall(r&quot;title=\&quot;(.+?)\&quot;&quot;,str(i))[0])#正则在comment标签中获取星级， return stardef conbineResult(t):#将每次爬取的结果写入文件中，多进程并发写入同一文件时会出现资源竞争，所以使用Pool类中的callback方法调用回调函数 with open(r&apos;D:/outp.txt&apos;,&apos;ab&apos;) as f:#注意pickle默认使用默认使用二进制存储，这里不能用a if t!=1: pickle.dump(t,f)#序列化存储结果 def main(): href=getRegion()#获得各个区域的链接 pool=multiprocessing.Pool(processes=4)#建立进程池,默认为cpu核数 print(href) print(&apos;区域数量为&apos;+str(len(href))) for i in range(len(href)): for index in range(1,3):#取1到5面的内容 if index==1: realurl=href[i] else: realurl=href[i]+&apos;p&apos;+str(index)#注意到每面的链接就是在第一面的基础上加上p+页数 print(realurl) result=pool.apply_async(getValue,(realurl,),callback=conbineResult)#添加进程，并且调用回调函数conbineResult处理结果 pool.close()#关闭进程池，不再添加进程 pool.join()#因为是非阻塞的，主进程继续执行直到遇到.join()等待pool中所有子进程完毕，主进程才继续 if result.successful(): print(&apos;successful&apos;) &apos;&apos;&apos; with open(r&apos;D:/outp.txt&apos;,&apos;rb&apos;) as f:#对于多次dump,pickle每次序列化生成的字符串有独立头尾,pickle.load()只会读取一个完整的结果,需要循环读出 try: while True: datas=pickle.load(f)#取出数据 print(datas) except: print(&apos;success&apos;) &apos;&apos;&apos;if __name__ == &quot;__main__&quot;: main() 参考文献 Python爬虫利器二之Beautiful Soup的用法 | 静觅 python多进程的理解 multiprocessing Process join run – 李皮筋 – 博客园 使用python+selenium+Firefox登录大众点评 – wmq104的博客 – CSDN博客 Python爬虫模拟登录的黑魔法 – 简书]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之浅谈万物皆对象(1)]]></title>
    <url>%2F2019%2F02%2F26%2FJava%E4%B9%8B%E6%B5%85%E8%B0%88%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1-1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;“如果我们说另一种不同的语言，那么我们就会发觉一个有些不同的世界。”&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;——Luduing Wittgerstein（1889-1951） &emsp;&emsp;尽管Java是基于C++的，但是相比之下，Java是一种更“纯粹”的面向对象程序设计语言（OOP），即“万物皆对象”。 面向过程和面向对象的区别&emsp;&emsp;面向过程和面向对象是编程中两个非常基本的思想。很多人之前学过的C语言就是面向过程的语言，而C++、Java等就是面向对象的语言。目前显然面向对象是一种主流的语言，工作和实习中也以面向对象为主。 &emsp;&emsp;首先，面向过程和面向对象都是软件分析、设计和开发的一种思想，它指导着人们以不同的方式去分析、设计和开发软件。但这两者并不对立，而是相辅相成的关系，我们去写一个软件，既可以用到面向对象的思想，也可以用到面向过程的思想。 &emsp;&emsp;那么什么叫面向过程思想？当你碰到一些简单的问题时，我们会自然而然地使用面向过程的思想。比如说你碰到一件事，你会想你第一步应该怎么做，第二步应该怎么做，第三步应该怎么做……当你这么想的时候，你就自然而然地使用了面向过程的思想。举一个简单的例子，如何开车？很多人去学车，学车就会面对一个问题，一辆车怎么发动起来，就会考虑：第一步点火，第二步踩离合，第三步踩油门，这个过程就是面向过程。 &emsp;&emsp;面向过程适合简单、不需要协作的事务。就比如之前提到的开车，开车不需要协助，你自己一个人就够了，这就叫简单的事务。但是当我们去思考比较复杂的问题，比如如何造车？这个时候你就会发现需要很多人去协作，很难明确列出几个步骤。这个时候就需要用面向对象的思想。现在问你如何造车，一个正常人都不会去思考我第一步造什么，第二步造什么，而是首先去思考这车都由哪些物件（Object）组成，用一种设计的眼光去看问题。车是由车窗、轮胎、发动机等部件组成，思考如何造车，我们就会去思考怎么造这些部件。 &emsp;&emsp;由此可见，面对复杂的问题，我们会自然而然地去使用面向对象的思想，所以面向对象的思想适合开发一些大型的软件。 &emsp;&emsp;面向对象（Object）思想更契合人的思维模式。那么回到之前的造车问题，我们会发现，造轮胎、造发动机也是有步骤的，也就是说，面向对象和面向过程是相辅相成的。面向对象在宏观上把握整体的软件设计，微观上仍然需要使用面向过程。非常典型的问题就是，我们编程永远离不开方法，而方法就是典型的面向过程的产物。 面向对象和面向过程的总结： 都是解决问题的思维方式，都是代码组织的方式 解决简单问题可以使用面向过程 解决复杂问题：宏观上使用面向对象把握，微观处理上仍然是面向过程]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总得写点什么]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%80%BB%E5%BE%97%E5%86%99%E4%BA%9B%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[其实，我根本没尽全力做过一件事&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;好像每一个拥有技术博客的人，都会在开篇写一写为什么自己要建一个技术博客。就好比程序员在学一门新的语言，第一段代码总要写一个Hello World，以表示对这个世界的尊重。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么我为什么要建这样一个Blog呢？大家都这样做，那我也跟个潮流？或者把自己掌握的知识写出来向小伙伴们炫耀？都不是。我只是想尽全力做一件事，从写Blog开始。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去七千多个日日夜夜幻灯片似的在我的眼前滑过： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;早上8点半，闹钟响了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我磨蹭了20分钟，不情不愿地爬起了床。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我决定，今天一定要看完半本专业书，把简历好好润色一下，再写两篇心得。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我坐在书桌前，翻了10分钟的书后，就掏出了手机。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我想，没关系，我就刷一下朋友圈。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果，这一天，我刷了一小时的朋友圈，看了三小时的剧，翻了两个多小时的微博，打了半天的王者农药。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看一眼时间，怎么又要到零点了？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我觉得很懊恼，开始自责，犹豫要不要熬夜把这些事做了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过一会，我想想算了，晚上效率也不高，还是明天再继续吧。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是，我躺到床上，开始玩起了手机。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“别难过，你已经尽力啦。” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做一些事失败后，总会收到这样的安慰。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;听到这句话，觉得宽慰的同时，总会有点心虚： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我真的尽力了吗？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，仔细回想，那些我说自己尽力了的时候，好像都不算尽力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;想考研能考上个好学校，知道微积分是弱项，但也只是嘴上说着要好好复习微积分，带回来的课本还躺在行李箱里蒙尘。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;希望拥有一份自己喜欢的工作，也只是在daydreaming时幻想一下在BAT工作时的场景，根本没有认真地去规划和实习。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;喜欢上一个女孩，因为怕被拒绝，所以只敢有事没事点个赞，发句不痛不痒的晚安，却从不去真的表白。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只是，为了让自己从悔恨的苦海里挣脱出来，也就顺势接受了这样的宽慰。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但在心里那个不为人知的回放室里，每每想到那些虚度的时光，总会有些羞愧，又会莫名感到松了口气： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你看，这些其实都是因为我没尽力，才做不到的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在尽力这件事上，我们总是异常地谨慎。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于很多人来说，相比于失败或努力后失败，更可怕的可能是———— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用尽全力去做一件事，然后失败了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就像是在昭示着： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你没天赋的，你不行的，你根本就不是做这块的料。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种挫败感，远远胜过于没怎么尽力而失败的挫败感，并且总会让人觉得，当初用尽全力的自己，仿佛就像是个二傻子。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是，大部分时候，我们只是象征性地努力一下，然后就躺平认输了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但嘴上还是会喃喃着，安抚自己和身边的人： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;啊，没事，我已经尽力了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;几个月前，看了一部很暖也很热血的日剧，名字叫《重版出来》，讲的是一群漫画社的成员为了让漫画“再次印刷”的故事。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主角黑泽是一个元气满满的少女，因为喜欢漫画选择去做了一个漫画编辑，做每一件事都会用尽全力，不管它看上去多不可实现、无趣枯燥。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与之完全相反的是销售部的小泉君。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;他想成为一个编辑，却意外被分到销售部，每天都陷入丧气和绝望之中，销售额时常垫底，因为存在感太低，被称作为“幽灵”。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;转折点在一次推广的任务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;黑泽被派去帮忙小泉君，销售一本毫无名气、销量惨淡的漫画书。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;黑泽主动与书店店员说明这本书的创作初衷和现实情况，希望作者能在这家书店开签售会。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了增强漫画的曝光率，黑泽还向店员提出，在相关的区域设置漫画专栏，以增加二者的销售额。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果，这本漫画深受读者的喜欢，卖得很火，真的“重版出来”了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;小泉君被黑泽的干劲所感染，叹道： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看到这的时候，虽然也并非多新颖的励志桥段，但还是被戳中了泪点。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;仔细想想，大概是因为我和小泉君一样，从未体会过“用尽全力”是一种怎样的感受。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们总会去思考———— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这件事成功的概率会有多少？它值得投入这么多吗？如果尽力后，还失败，该怎么办？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而在这些日剧里，那些我们不敢有的“尽力生活”全都被展现了出来。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在看到主角不顾一切往前冲的背影时，才恍然发现： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原来，用尽全力、不计后果地去尝试，一点也不傻啊。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，这世上很少有人能说： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一生，我用尽全力去活了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大部分人都只是畏畏缩缩、拖拖拉拉地过完了大半辈子，然后在快离世的时候，才遗憾感叹，一辈子竟然就这样结束了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;日本作家中岛敦在《山月记》里写过一段很扎心的话： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因为害怕自己并非明珠而不敢刻苦琢磨，又因为有几分相信自己是明珠，而不能与瓦砾碌碌为伍…… &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如今想起来，我真是空费了自己那一点仅有的才能，徒然在口头上卖弄着什么“人生一事不为则太长，欲为一事则太短”的警句，可事实是，唯恐暴露才华不足的卑怯的畏惧，和厌恶钻研刻苦的惰怠，就是我的全部了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，在出壳之前，谁也不知道自己是明珠还是瓦砾。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可是，是明珠还是瓦砾，又有这么重要吗？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关键是，我们这只有一次的人生———— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本来就应该，用尽全力、半点不浪费地去活啊。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
