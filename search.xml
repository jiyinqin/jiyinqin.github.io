<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java之浅谈万物皆对象(2)]]></title>
    <url>%2F2019%2F03%2F03%2FJava%E4%B9%8B%E6%B5%85%E8%B0%88%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[什么是对象？&emsp;&emsp;对象（Object）可以看作一种数据管理的方式。我们可以把对象理解为内存中的一个内存块，其中包含很多数据。 &emsp;&emsp;那么这个内存块都包含什么样的数据？为什么要这样包含？ &emsp;&emsp;在程序最初发展的时候是不需要对象的，就比如我们只需要写一个简单的加法、减法，我们只需要基本数据类型就够了。那为什么后来需要对象呢？很简单：事物的发展总是遵循“量变引起质变”。当一个量达到一个很高的值的时候，我们就会发现目前的系统、方式就适应不了事情的发展了。就比如牛顿体系，三百年前显得非常完美，但放在今天我们知道，当物体运动速度达到光速的时候，牛顿体系就存在问题了，这就是量变引起质变，所以才有了爱因斯坦的相对论。对于企业、社会、计算机也是这样。 &emsp;&emsp;回到之前的话题。我们刚才说到在程序最初的时候是不需要对象的，因为我们需要的数据少。所以我在这里把我们对象的数据管理的发展分为三个阶段： 数据无管理时代&emsp;&emsp;最初的计算机语言只有基本变量(类似我们学习的基本数据类型)，用来保存数据。那时候面对的数据非常简单，只需要几个变量即可搞定;这个时候不涉及“数据管理”的问题。同理，就像在企业最初发展阶段只有几个人，不涉及管理问题，大家闷头做事就OK了。 数组管理和企业部门制&emsp;&emsp;企业发展中，员工多了怎么办?我们很自然的想法就是归类，将类型一致的人放到一起;企业中，会将都做销售工作的放到销售部管理;会将研发软件的放到开发部管理。同理在编程中，变量多了，我们很容易的想法就是“将同类型数据放到一起”， 于是就形成了“数组”的概念，单词对应“array”。 这种“归类”的思想，便于管理数据、管理人。 对象和企业项目制&emsp;&emsp;企业继续发展，面对的场景更加复杂。一个项目可能需要经常协同多个部门才能完成工作;一个项目从谈判接触可能需要销售部介入;谈判完成后，需求调研开始，研发部和销售部一起介入;开发阶段需要开发部和测试部互相配合敏捷开发，同时整个过程财务部也需要跟进。在企业中，为了便于协作和管理，很自然就兴起了“项目制”，以项目组的形式组织，一个项目组可能包含各种类型的人员。 一个完整的项目组，麻雀虽小五脏俱全，就是个创业公司甚至小型公司的编制，包含行政后勤人员、财务核算人员、开发人员、售前人员、售后人员、测试人员、设计人员等等。事实上，华为、腾讯、阿里巴巴等大型公司内部都是采用这种“项目制”的方式进行管理。 &emsp;&emsp;同理，计算机编程继续发展，各种类型的变量更加多了，而且对数据的操作(指的就是方法，方法可以看做是对数据操作的管理)也复杂了，怎么办? &emsp;&emsp;为了便于协作和管理，我们“将相关数据和相关方法封装到一个独立的实体”，于是“对象”产生了。 比如，我们的一个学生对象： &emsp;&emsp;有属性(静态特征)：年龄：20，姓名：老鸡，学号：009 &emsp;&emsp;也可以有方法(动态行为)：学习，吃饭，睡觉，上课 总结 对象说白了也是一种数据结构(对数据的管理模式)，将数据和数据的行为放到了一起 在内存上，对象就是一个内存块，存放了相关的数据集合 对象的本质就一种数据的组织方式]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你上次真正开心是什么时候？]]></title>
    <url>%2F2019%2F03%2F02%2F%E4%BD%A0%E4%B8%8A%E6%AC%A1%E7%9C%9F%E6%AD%A3%E5%BC%80%E5%BF%83%E6%98%AF%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;开学前和学长出去喝酒。 &emsp;&emsp;酒过三巡，开始聊到最近的状况。 &emsp;&emsp;“没什么不开心的，也没什么开心的。” &emsp;&emsp;学长说着，又倒了一杯酒。 &emsp;&emsp;他前阵子刚找到一份大厂的工作，遇到的上司不错，和同事之间的关系也算融洽，但不知道为什么—— &emsp;&emsp;“就是提不起劲来，感觉没意思。” &emsp;&emsp;不止是学长。 &emsp;&emsp;那天一起喝酒的老刑和蛋蛋也有这个感受。 &emsp;&emsp;老刑刚找到一份投行的实习，蛋蛋在和朋友一起接外包。 &emsp;&emsp;平时在朋友圈里，他们发的生活片段或文艺或高大上，看上去活得充实而有趣。 &emsp;&emsp;但他们大部分时间其实也不怎么开心。 &emsp;&emsp;“有时候我也怀疑是不是自己太作了。明明现在做的是自己喜欢做的事情，之前也一直在为能以此谋生而努力。但真的做了一段时间却觉得： &emsp;&emsp;啊，日子过得好慢呀。” &emsp;&emsp;老邢叹道。 &emsp;&emsp;“所以，你上次真的感到开心是什么时候？” &emsp;&emsp;在被学长反复问这个问题的时候，我愣了大概两分钟。 &emsp;&emsp;那两分钟里，我试图搜寻出关于快乐的记忆，但时间线却好像都有点久远。 &emsp;&emsp;在占比很大的记忆里，充斥着无聊、焦虑、孤独，以及琐碎的因为看剧、刷图片而引起的笑意。 &emsp;&emsp;事实上，在学长问我之前，我也已经隐约有了这样的感觉： &emsp;&emsp;好像，现在越来越难开心了。 &emsp;&emsp;每本好看的小说快结束时，会极度惋惜和害怕，为了避免陷入长久的空虚里，要赶快去找下一本。 &emsp;&emsp;在朋友圈里刷到有趣的段子后，会不由自主地继续刷新下去来消磨时间。 &emsp;&emsp;完成一个项目后，希望能马上开始下一个项目，以求获得价值和存在感。 &emsp;&emsp;一切仿佛陷入了一个无解的死循环。 &emsp;&emsp;不过，很奇怪的，当我意识到自己其实“没那么开心”时，反倒觉得松了口气。 &emsp;&emsp;就像是之前一直觉得身体不对劲，但不知道“出毛病”的地方在哪里，去医院做了检查才知道，原来是哪里和哪里出了问题。 &emsp;&emsp;事实上，最可怕的可能是，很多时候，我们都没意识到—— &emsp;&emsp;原来自己没觉得太开心。 &emsp;&emsp;如果一切都很正常，一切也都往好的方向发展，那么，好像没有理由不开心才对？ &emsp;&emsp;但不是的。 &emsp;&emsp;一切都正常只能让我们可以避免“痛苦”。如果没那么开心的话，可能意味着，现在的生活缺少了目标和新鲜感。 &emsp;&emsp;而只有意识到这些，我们才能给自己去创造这些东西，从而让自己快乐。 &emsp;&emsp;我们都太轻视“开心”这件事了。 &emsp;&emsp;或者说，我们都太轻视“好好生活”这件事。 &emsp;&emsp;我们很少去问自己： &emsp;&emsp;你今天开心吗？ &emsp;&emsp;你今天过的好吗？ &emsp;&emsp;于是我们只是浑浑噩噩地去起床，浑浑噩噩地上床。 &emsp;&emsp;然后一晃眼，几十年就这样过去了。 &emsp;&emsp;那么不妨，从明天开始认真审视自己，并保持新鲜和很好奇： &emsp;&emsp;送自己一朵好看的花，学做一个好吃的甜点，看原来没看过的书，走原来没走过的路，去以前没去过的小店逛逛，认识完全没接触过的人。 &emsp;&emsp;就像是《今天也要用心过生活》里，松浦弥太郎说的： &emsp;&emsp;只要日子过得新鲜有趣，人就有仔细过生活的动力。 &emsp;&emsp;在做完这些后，即使没有觉得快乐，也一定变成了一个更懂得生活的人了。 &emsp;&emsp;然后，一定有一天，快乐会自己找上门来。 &emsp;&emsp;“你上次真正开心是什么时候？” &emsp;&emsp;“上一秒。”]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实战之飞机游戏项目]]></title>
    <url>%2F2019%2F03%2F01%2FJava%E5%AE%9E%E6%88%98%E4%B9%8B%E9%A3%9E%E6%9C%BA%E6%B8%B8%E6%88%8F%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;这两天我利用所学知识写了这一款飞机小游戏，作为Java的实战练习。该小游戏的目标是：用户通过键盘可以控制飞机前后移动，躲避炮弹，看谁坚持的时间长。如果碰到炮弹则发生爆炸，游戏结束，并显示本次生存的时间和等级排名。以下是游戏的具体代码。游戏代码存放在包cn.sxt.game中 基于AWT技术的游戏主窗口 MyGameFrame.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160package cn.sxt.game;import java.awt.Color;import java.awt.Font;import java.awt.Frame;import java.awt.Graphics;import java.awt.Image;import java.awt.Rectangle;import java.awt.event.KeyAdapter;import java.awt.event.KeyEvent;import java.awt.event.WindowAdapter;import java.awt.event.WindowEvent;import java.util.Date;import javax.swing.JFrame;/** * 飞机游戏的主窗口 * @author 纪尹钦 * */public class MyGameFrame extends Frame &#123; Image planeImg = GameUtil.getImage(&quot;images/plane.jpg&quot;); Image bg = GameUtil.getImage(&quot;images/bg.jpg&quot;); Plane plane = new Plane(planeImg,250,250); Shell[] shells = new Shell[40]; Explode bao; Date startTime = new Date(); Date endTime; int period; //游戏持续的时间 @Override public void paint(Graphics g) &#123; //自动被调用 g相当于一只画笔 super.paint(g); Color c = g.getColor(); g.drawImage(bg, 0, 0, null); plane.drawSelf(g); //画飞机 //画出所有的炮弹 for(int i=0;i&lt;shells.length;i++) &#123; shells[i].draw(g); //飞机和炮弹的碰撞检测 boolean peng = shells[i].getRect().intersects(plane.getRect()); if(peng) &#123; plane.live = false; if(bao == null)&#123; bao = new Explode(plane.x, plane.y); endTime = new Date(); period = (int)((endTime.getTime()-startTime.getTime())/1000); &#125; bao.draw(g); &#125; //计时工能，给出提示 if(!plane.live) &#123; g.setColor(Color.red); Font f = new Font(&quot;宋体&quot;,Font.BOLD,50); g.setFont(f); g.drawString(&quot;时间：&quot;+period+&quot;秒&quot;, (int)plane.x, (int)plane.y); &#125; &#125; g.setColor(c); &#125; //帮助我们反复地重画窗口 class PaintThread extends Thread &#123; @Override public void run() &#123; while(true) &#123; repaint(); //重画 try &#123; Thread.sleep(40); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125; //定义键盘监听的内部类 class KeyMonitor extends KeyAdapter &#123; @Override public void keyPressed(KeyEvent e) &#123; plane.addDirection(e); &#125; @Override public void keyReleased(KeyEvent e) &#123; plane.minusDirection(e); &#125; &#125; /** * 初始化窗口 */ public void launchFrame() &#123; this.setTitle(&quot;和老鸡一起来van游戏吧！&quot;); this.setVisible(true); this.setSize(Constant.GAME_WIDTH,Constant.GAME_HEIGHT); this.setLocation(300, 300); this.addWindowListener(new WindowAdapter() &#123; @Override public void windowClosing(WindowEvent e) &#123; System.exit(0); &#125; &#125;); new PaintThread().start();//启动重画窗口的线程 addKeyListener(new KeyMonitor());//给窗口增加键盘的监听 //初始化50个炮弹 for(int i=0;i&lt;shells.length;i++) &#123; shells[i]=new Shell(); &#125; &#125; public static void main(String[] args) &#123; MyGameFrame f = new MyGameFrame(); f.launchFrame(); &#125; private Image offScreenImage = null; public void update(Graphics g) &#123; if(offScreenImage == null) offScreenImage = this.createImage(Constant.GAME_WIDTH,Constant.GAME_HEIGHT);//这是游戏窗口的宽度和高度 Graphics gOff = offScreenImage.getGraphics(); paint(gOff); g.drawImage(offScreenImage, 0, 0, null); &#125; &#125; 用来存放常量的类 Constant.java123456package cn.sxt.game;public class Constant &#123; public static final int GAME_WIDTH = 535; public static final int GAME_HEIGHT = 535;&#125; 游戏物体的父类 GameObject.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package cn.sxt.game;import java.awt.Graphics;import java.awt.Image;import java.awt.Rectangle;/** * 游戏物体的父类 * @author 纪尹钦 * */public class GameObject &#123; Image img; double x,y; int speed; int width,height; public void drawSelf(Graphics g) &#123; g.drawImage(img, (int)x, (int)y, null); &#125; public GameObject(Image img, double x, double y, int speed, int width, int height) &#123; super(); this.img = img; this.x = x; this.y = y; this.speed = speed; this.width = width; this.height = height; &#125; public GameObject(Image img, double x, double y) &#123; super(); this.img = img; this.x = x; this.y = y; &#125; public GameObject() &#123; &#125; /** * 返回物体所在的矩形，便于后续的碰撞检测 * @return */ public Rectangle getRect() &#123; return new Rectangle((int)x, (int)y, width, height); &#125; &#125; 工具类 GameUtil.java123456789101112131415161718192021222324252627282930package cn.sxt.game; import java.awt.Image;import java.awt.image.BufferedImage;import java.io.IOException;import java.net.URL;import javax.imageio.ImageIO; public class GameUtil &#123; // 工具类最好将构造器私有化。 private GameUtil() &#123; &#125; /** * 返回指定路径文件的图片对象 * @param path * @return */ public static Image getImage(String path) &#123; BufferedImage bi = null; try &#123; URL u = GameUtil.class.getClassLoader().getResource(path); bi = ImageIO.read(u); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return bi; &#125;&#125; 飞机类 Plane.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package cn.sxt.game;import java.awt.Graphics;import java.awt.Image;import java.awt.event.KeyEvent;public class Plane extends GameObject&#123; boolean left,up,right,down; boolean live = true; public void drawSelf(Graphics g) &#123; if(live) &#123; g.drawImage(img, (int)x, (int)y, null); if(left) &#123; x -= speed; &#125; if(right) &#123; x += speed; &#125; if(up) &#123; y -= speed; &#125; if(down) &#123; y += speed; &#125; &#125; &#125; public Plane(Image img,double x,double y) &#123; this.img=img; this.x=x; this.y=y; this.speed=4; this.width=img.getWidth(null); this.height=img.getHeight(null); &#125; //按下某个键，增加相应的方向 public void addDirection(KeyEvent e) &#123; switch (e.getKeyCode()) &#123; case KeyEvent.VK_LEFT: left=true; break; case KeyEvent.VK_UP: up=true; break; case KeyEvent.VK_RIGHT: right=true; break; case KeyEvent.VK_DOWN: down=true; break; &#125; &#125; //按下某个键，取消相应的方向 public void minusDirection(KeyEvent e) &#123; switch (e.getKeyCode()) &#123; case KeyEvent.VK_LEFT: left=false; break; case KeyEvent.VK_UP: up=false; break; case KeyEvent.VK_RIGHT: right=false; break; case KeyEvent.VK_DOWN: down=false; break; &#125; &#125;&#125; 炮弹类 Shell.java1234567891011121314151617181920212223242526272829303132333435363738394041424344package cn.sxt.game;import java.awt.Color;import java.awt.Graphics;/** * 炮弹类 * @author 纪尹钦 * */public class Shell extends GameObject &#123; double degree; public Shell() &#123; x=200; y=200; width=10; height=10; speed=3; degree = Math.random()*Math.PI*2; &#125; public void draw(Graphics g) &#123; Color c = g.getColor(); g.setColor(Color.YELLOW); g.fillOval((int)x, (int)y, width, height); //炮弹沿着任意角度去飞 x += speed*Math.cos(degree); y += speed*Math.sin(degree); if(x&lt;0||x&gt;Constant.GAME_WIDTH-width) &#123; degree=Math.PI-degree; &#125; if(y&lt;40||y&gt;Constant.GAME_HEIGHT-height) &#123; degree=-degree; &#125; g.setColor(c); &#125; &#125; 爆炸类 Explode.java1234567891011121314151617181920212223242526272829303132package cn.sxt.game;import java.awt.Graphics;import java.awt.Image;/* * 爆炸类 */public class Explode &#123; double x,y; static Image[] imgs = new Image[16]; static &#123; for(int i=0;i&lt;16;i++)&#123; imgs[i] = GameUtil.getImage(&quot;images/explode/e&quot;+(i+1)+&quot;.gif&quot;); imgs[i].getWidth(null); &#125; &#125; int count; public void draw(Graphics g)&#123; if(count&lt;=15)&#123; g.drawImage(imgs[count], (int)x, (int)y, null); count++; &#125; &#125; public Explode(double x,double y)&#123; this.x = x; this.y = y; &#125;&#125; 游戏所需图片素材&emsp;&emsp;游戏所需要的素材都已打包存入百度云中，需要自提 &emsp;&emsp;链接：https://pan.baidu.com/s/1BmI7yTZBr8s4UqepHCuuQQ &emsp;&emsp;提取码：p05l 总结&emsp;&emsp;以上就是飞机游戏的全部代码，达到了一开始定下的目标。由于时间仓促，图形界面还较为简陋，细节部分有待优化。最后成品效果如下图所示：]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫实战之大众点评]]></title>
    <url>%2F2019%2F02%2F26%2FPython%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%E4%B9%8B%E5%A4%A7%E4%BC%97%E7%82%B9%E8%AF%84%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;大二上学期的综合程序设计，挑了个应该最水的爬虫（懒得花功夫在这个水课上Orz），结果最后一直从开学咕咕咕到期末，考试周两天抽空把才敲完了（严重导致编译技术没复习好o(╥﹏╥)o），在队友也是鸽子的情况下，拖到最后果然不是个好习惯2333。好吧废话少说，开始正题简短地记录下。 网站分析&emsp;&emsp;这次我们选取的是Web端的大众点评，准备爬取的信息是商铺的信息，首先爬取区域链接，再分区域根据其智能排序爬取他的店名，id，链接，星级。 &emsp;&emsp;本身网页的分析并不困难，比较麻烦的点是大众点评的反爬机制有点麻烦，需要综合下各种方法。然后就是用多进程对爬虫的性能进行了下提升，最后还临时补了一个登陆模块。 &emsp;&emsp;之后分析我们要爬取的数据时静态的，没有经过js渲染所以会简单一点（如果有动态渲染的话，应该就是利用selenium来进行模拟好做），直接用requests+Beautifulsoup进行网页的爬取和分析，我们观察它的前端页面： &emsp;&emsp;观察他的id=”bussi-nav”，然后在这个div下遍历下他的下级标签，正则匹配出链接href，存储在一个list中并返回. 123456789101112131415def getRegion():#返回要爬取的区域的链接,返回一个数组 href=[]#用来存储每个区域的链接 url=&quot;http://www.dianping.com/chengdu/ch10/r1579&quot; try: soup=UseBeautifulSoup(url) s=soup.find_all(id=&quot;bussi-nav&quot;)[0]#注意到区域的div的id是bussi-nav,find_all方法筛选到 for m in s.select(&apos;a&apos;): href.append(re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(m))[0])#正则找到链接，存储到href数组中,注意这里re.fnidall返回的是一个数组,取第一项 print(&apos;爬取地区链接成功&apos;) return href except: print(&apos;爬取地区链接失败&apos;) print(soup) os._exit() &emsp;&emsp;之后看店铺的: &emsp;&emsp;观察它的a标签中data-click-name=”shop_title_click”但是这里不能直接用这个tag属性进行搜索，需要用attrs方法转换为键值对，然后遍历这个标签下每个店铺的名称 ，链接，并从链接中取出唯一id存放在dict中，然后看星级的class=’comment’，同样遍历存储在一个dict中，最后再与前面的信息进行匹配，放在一个字典中。 1234567891011121314151617181920212223242526272829@retry(retry_on_result=retry_if_result_none)#当出现错误（没有爬取到star）时，进行异常重试def getValue(url):#获取当面的店名以及他的id，链接，星级，返回一个字典 name_And_value=&#123;&#125; value=&#123;&apos;id&apos;:&apos;&apos;,&apos;href&apos;:&apos;&apos;,&apos;star&apos;:&apos;&apos;&#125; count=0 try: soup=UseBeautifulSoup(url) star=get_Star(soup)#获得这面的star if len(star)==0:#没有爬到，返回个None return None shoplist=soup.find_all(&apos;a&apos;,attrs=&#123;&quot;data-click-name&quot;:&quot;shop_title_click&quot;&#125;)#注意这里有些tag属性是不能用来搜索的，例如HTML5中的data-*属性，我们需要用attrs方法 for i in shoplist: temp=value.copy()#这里我遇到一个坑点，在字典1中添加字典2其指向的应该是被添加字典2的地址，并没有新建一个对象，所以在循环汇总改变字典2的值，有点类似深浅拷贝，也会改变字典1里面的值具体参考：https://blog.csdn.net/sinat_21302587/article/details/72356431 name=re.findall(r&quot;title=\&quot;(.+?)\&quot;&quot;,str(i))[0]#正则寻找店名及 temp[&apos;href&apos;]=re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(i))[0]#链接存储 temp[&apos;id&apos;]=re.findall(r&quot;http://www.dianping.com/shop/(.+)&quot;,str(temp[&apos;href&apos;]))[0]#获取每个店铺的id值 temp[&apos;star&apos;]=star[count]#存入对应的星级 name_And_value[name]=temp count+=1 return name_And_value except: print(&apos;爬取区域店铺失败&apos;) print(soup) os._exit() return 1 &emsp;&emsp;这个基本上就是爬取信息的最基本流程，但是这仅仅是个开始Orz，最重要的还是怎么绕过他的反爬机制。 反爬虫分析&emsp;&emsp;经过我试了无数次的结果，大概上猜测出大众点评的反爬逻辑，大概有这几点： 一定要带上headers，否则直接报403 需要不断更改user-agent,referer，显著提高成功率，尤其是不断更换referer后效果明显 根据ip的访问频率，否则会200报一个访问页面不存在（但是使用代理ip后效果不是特别明显，怀疑主要是根据cookie） 根据cookie的访问频率（八成把握），效果同上 某个cookie或者ip访问达到一定次数（频率）会跳验证码，并且还是双重的，第一个是个拖动，第二个是个图像 不同的页面反爬虫的程度不同，商家店铺页面会更严格（猜测的，在爬取测试的时候，但是也有可能跟爬取的量有关，还没有达到他的阈值） &emsp;&emsp;所以针对以上问题，就需要构建一个ua池，构建一个referer池，本来还应该构建一个ip池，但是那些免费的代理网站的ip实在不稳定，筛选下来的成功率太低了，就直接使用了付费ip代理，这里我选择是阿布云代理，一块钱一小时，因为这次的爬虫不是很大所以也可以接受（下次试试自己搭建ip池），其实在这里爬虫的成功率就已经很高了估计有个七八成，基本上达到我们的要求，所以就没有搭建cookie池（可能搭建个cookie池会更好） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def get_proxy():#使用阿布云的代理接口 # 要访问的目标页面 targetUrl = &quot;http://test.abuyun.com&quot; # 代理服务器 proxyHost = &quot;http-dyn.abuyun.com&quot; proxyPort = &quot;9020&quot; # 代理隧道验证信息 proxyUser = &quot;HS9039NV974Y34KD&quot; proxyPass = &quot;B5687CE5E8CFD090&quot; proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123; &quot;host&quot; : proxyHost, &quot;port&quot; : proxyPort, &quot;user&quot; : proxyUser, &quot;pass&quot; : proxyPass, &#125; proxies = &#123; &quot;http&quot; : proxyMeta, &quot;https&quot; : proxyMeta, &#125; return proxiesdef get_ua():#构建ua池 user_agents = [ &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&apos;, &apos;Opera/8.0 (Windows NT 5.1; U; en)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&apos;, &apos;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2 &apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&apos;, &apos;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&apos;, &apos;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0&apos;, &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0) &apos;, ] user_agent = random.choice(user_agents) #random.choice(),从列表中随机抽取一个对象 return user_agentdef get_ref():#构建referer池 referers=[&apos;http://www.dianping.com/chengdu/ch10/r7949&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1577&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7767&apos;, &apos;http://www.dianping.com/chengdu/ch10/r5894&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1592&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1601&apos;, &apos;http://www.dianping.com/chengdu/ch10/r70146&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7764&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7771&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7769&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1597&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7768&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1974&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1578&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1604&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1579&apos; ] referer=random.choice(referers) return referer@retry#利用retry装饰器，异常重试def UseBeautifulSoup(url):#使用beautifulsoup处理 ua=get_ua()#获得随机的User-Agent referer=get_ref()#获得随机的Referer proxy=get_proxy()#获得代理 with open(&apos;D:/cookie.ini&apos;, &apos;rb&apos;) as f:#读取cookie data = pickle.load(f) for cookie in data: cookies=&#123;cookie[&apos;name&apos;]:cookie[&apos;value&apos;]&#125; headers=&#123; &quot;User-Agent&quot;:ua, &quot;Referer&quot;:referer, &quot;Cookie&quot;:str(cookies),#这里需要str()下 &#125;#这里需要加个header，否则直接给返回个403 res=requests.get(url,headers=headers,proxies=proxy)#使用代理 res.encoding=&apos;utf-8&apos; soup=BeautifulSoup(res.text,&apos;html.parser&apos;) return soup 代码优化&emsp;&emsp;之后就是对爬虫一点的优化了，这里我选择使用了多进程来进行爬取（Python的多线程虽然是伪多线程，但是爬虫的话也可以使用，因为爬虫是一个I/O密集型的，不是计算密集型的），然后由于代理ip接口可能不稳定啊，或者爬取失败，这里使用了retry模块，利用retry装饰器进行异常重试，最后用pickle序列化存储数据，之后数据交给队友存到数据库里（总要象征性的让他们有点参与感233）。 &emsp;&emsp;retry装饰器在前面一段代码，一下是使用多进程对爬取不同页面： 123456789101112131415161718192021222324252627282930313233343536def conbineResult(t):#将每次爬取的结果写入文件中，多进程并发写入同一文件时会出现资源竞争，所以使用Pool类中的callback方法调用回调函数 with open(r&apos;D:/outp.txt&apos;,&apos;ab&apos;) as f:#注意pickle默认使用默认使用二进制存储，这里不能用a if t!=1: pickle.dump(t,f)#序列化存储结果 def main(): href=getRegion()#获得各个区域的链接 pool=multiprocessing.Pool(processes=4)#建立进程池,默认为cpu核数 print(href) print(&apos;区域数量为&apos;+str(len(href))) for i in range(len(href)): for index in range(1,3):#取1到5面的内容 if index==1: realurl=href[i] else: realurl=href[i]+&apos;p&apos;+str(index)#注意到每面的链接就是在第一面的基础上加上p+页数 print(realurl) result=pool.apply_async(getValue,(realurl,),callback=conbineResult)#添加进程，并且调用回调函数conbineResult处理结果 pool.close()#关闭进程池，不再添加进程 pool.join()#因为是非阻塞的，主进程继续执行直到遇到.join()等待pool中所有子进程完毕，主进程才继续 if result.successful(): print(&apos;successful&apos;) &apos;&apos;&apos; with open(r&apos;D:/outp.txt&apos;,&apos;rb&apos;) as f:#对于多次dump,pickle每次序列化生成的字符串有独立头尾,pickle.load()只会读取一个完整的结果,需要循环读出 try: while True: datas=pickle.load(f)#取出数据 print(datas) except: print(&apos;success&apos;) &apos;&apos;&apos;if __name__ == &quot;__main__&quot;: main() 登录模块&emsp;&emsp;最后发现综设报告上还有个要求，需要加个登录模块，想想直接用selenium方便简单，也不需要分析包了。这里有个坑，在登录页面，密码登录界面需要切换下，它是内嵌了个iframe。 123456789101112131415161718192021222324252627282930313233from selenium import webdriverimport requestsimport timeimport pickledef main(): chromePath=r&apos;C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe&apos;#chromedriver的路径 wd = webdriver.Chrome(executable_path= chromePath)#构建浏览器 url=r&apos;https://account.dianping.com/login?redir=http%3A%2F%2Fwww.dianping.com%2F&apos; wd.get(url) iframe = wd.find_element_by_xpath(&apos;//*[@id=&quot;J_login_container&quot;]/div/iframe&apos;)#切换到登陆模块 wd.switch_to_frame(iframe) icon_pc = wd.find_element_by_xpath(&apos;/html/body/div/div[2]/div[5]/span&apos;)#选择账户登录 icon_pc.click() time.sleep(2) name_login = wd.find_element_by_xpath(&apos;//*[@id=&quot;tab-account&quot;]&apos;)#选择手机密码登录登录 name_login.click() time.sleep(2) wd.find_element_by_id(&apos;account-textbox&apos;).send_keys(&apos;**********&apos;)#输入手机号 wd.find_element_by_xpath(&apos;//input[@id=&quot;password-textbox&quot;]&apos;).send_keys(&apos;*******&apos;)#输入密码 wd.find_element_by_xpath(&apos;//button[@id=&quot;login-button-account&quot;]&apos;).click()#点击登录 cookies=wd.get_cookies()#cookies存储了登录后的cookie print(cookies) time.sleep(5) wd.quit()#退出浏览器 with open(&apos;D:/cookie.ini&apos;,&apos;wb&apos;) as f:#使用pickle序列化存储cookie pickle.dump(cookies,f)if __name__==&apos;__main__&apos;: main() &emsp;&emsp;最后将登录过后的cookie保存下来作为之后的请求cookie 总结&emsp;&emsp;主要的时间花在了如何绕过反爬虫，顺便了解了下Python的多线程，多进程，这次做的也还是很粗糙，有很多地方可以优化，大概对爬虫还有几个坑要填 多线程 验证码识别 scrapy框架&emsp;&emsp;最后贴上总的代码，登录模块代码前面已贴过，这里只放主程序。使用时要先跑下登录模块获得cookie再跑主程序。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188# -*- coding: UTF-8 -*-import requestsimport reimport randomfrom bs4 import BeautifulSoupfrom retrying import retryimport osimport multiprocessingimport pickledef get_proxy():#使用阿布云的代理接口 # 要访问的目标页面 targetUrl = &quot;http://test.abuyun.com&quot; # 代理服务器 proxyHost = &quot;http-dyn.abuyun.com&quot; proxyPort = &quot;9020&quot; # 代理隧道验证信息 proxyUser = &quot;HS9039NV974Y34KD&quot; proxyPass = &quot;B5687CE5E8CFD090&quot; proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % &#123; &quot;host&quot; : proxyHost, &quot;port&quot; : proxyPort, &quot;user&quot; : proxyUser, &quot;pass&quot; : proxyPass, &#125; proxies = &#123; &quot;http&quot; : proxyMeta, &quot;https&quot; : proxyMeta, &#125; return proxiesdef get_ua():#构建ua池 user_agents = [ &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60&apos;, &apos;Opera/8.0 (Windows NT 5.1; U; en)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0&apos;, &apos;Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2 &apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36&apos;, &apos;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11&apos;, &apos;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11&apos;, &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER&apos;, &apos;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)&apos;, &apos;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0&apos;, &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0) &apos;, ] user_agent = random.choice(user_agents) #random.choice(),从列表中随机抽取一个对象 return user_agentdef get_ref():#构建referer池 referers=[&apos;http://www.dianping.com/chengdu/ch10/r7949&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1577&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7767&apos;, &apos;http://www.dianping.com/chengdu/ch10/r5894&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1592&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1601&apos;, &apos;http://www.dianping.com/chengdu/ch10/r70146&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7764&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7771&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7769&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1597&apos;, &apos;http://www.dianping.com/chengdu/ch10/r7768&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1974&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1578&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1604&apos;, &apos;http://www.dianping.com/chengdu/ch10/r1579&apos; ] referer=random.choice(referers) return referer@retry#利用retry装饰器，异常重试def UseBeautifulSoup(url):#使用beautifulsoup处理 ua=get_ua()#获得随机的User-Agent referer=get_ref()#获得随机的Referer proxy=get_proxy()#获得代理 with open(&apos;D:/cookie.ini&apos;, &apos;rb&apos;) as f:#读取cookie data = pickle.load(f) for cookie in data: cookies=&#123;cookie[&apos;name&apos;]:cookie[&apos;value&apos;]&#125; headers=&#123; &quot;User-Agent&quot;:ua, &quot;Referer&quot;:referer, &quot;Cookie&quot;:str(cookies),#这里需要str()下 &#125;#这里需要加个header，否则直接给返回个403 res=requests.get(url,headers=headers,proxies=proxy)#使用代理 res.encoding=&apos;utf-8&apos; soup=BeautifulSoup(res.text,&apos;html.parser&apos;) return soup def getRegion():#返回要爬取的区域的链接,返回一个数组 href=[]#用来存储每个区域的链接 url=&quot;http://www.dianping.com/chengdu/ch10/r1579&quot; try: soup=UseBeautifulSoup(url) s=soup.find_all(id=&quot;bussi-nav&quot;)[0]#注意到区域的div的id是bussi-nav,find_all方法筛选到 for m in s.select(&apos;a&apos;): href.append(re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(m))[0])#正则找到链接，存储到href数组中,注意这里re.fnidall返回的是一个数组,取第一项 print(&apos;爬取地区链接成功&apos;) return href except: print(&apos;爬取地区链接失败&apos;) print(soup) os._exit()def retry_if_result_none(result): return result is None@retry(retry_on_result=retry_if_result_none)#当出现错误（没有爬取到star）时，进行异常重试def getValue(url):#获取当面的店名以及他的id，链接，星级，返回一个字典 name_And_value=&#123;&#125; value=&#123;&apos;id&apos;:&apos;&apos;,&apos;href&apos;:&apos;&apos;,&apos;star&apos;:&apos;&apos;&#125; count=0 try: soup=UseBeautifulSoup(url) star=get_Star(soup)#获得这面的star if len(star)==0:#没有爬到，返回个None return None shoplist=soup.find_all(&apos;a&apos;,attrs=&#123;&quot;data-click-name&quot;:&quot;shop_title_click&quot;&#125;)#注意这里有些tag属性是不能用来搜索的，例如HTML5中的data-*属性，我们需要用attrs方法 for i in shoplist: temp=value.copy()#这里我遇到一个坑点，在字典1中添加字典2其指向的应该是被添加字典2的地址，并没有新建一个对象，所以在循环汇总改变字典2的值，有点类似深浅拷贝，也会改变字典1里面的值具体参考：https://blog.csdn.net/sinat_21302587/article/details/72356431 name=re.findall(r&quot;title=\&quot;(.+?)\&quot;&quot;,str(i))[0]#正则寻找店名及 temp[&apos;href&apos;]=re.findall(r&quot;href=\&quot;(.+?)\&quot;&quot;,str(i))[0]#链接存储 temp[&apos;id&apos;]=re.findall(r&quot;http://www.dianping.com/shop/(.+)&quot;,str(temp[&apos;href&apos;]))[0]#获取每个店铺的id值 temp[&apos;star&apos;]=star[count]#存入对应的星级 name_And_value[name]=temp count+=1 return name_And_value except: print(&apos;爬取区域店铺失败&apos;) print(soup) os._exit() return 1def get_Star(soup):#获取商户的星级 star=[] comment=soup.find_all(&apos;div&apos;,class_=&apos;comment&apos;)#先找到comment标签 for i in comment: star.append(re.findall(r&quot;title=\&quot;(.+?)\&quot;&quot;,str(i))[0])#正则在comment标签中获取星级， return stardef conbineResult(t):#将每次爬取的结果写入文件中，多进程并发写入同一文件时会出现资源竞争，所以使用Pool类中的callback方法调用回调函数 with open(r&apos;D:/outp.txt&apos;,&apos;ab&apos;) as f:#注意pickle默认使用默认使用二进制存储，这里不能用a if t!=1: pickle.dump(t,f)#序列化存储结果 def main(): href=getRegion()#获得各个区域的链接 pool=multiprocessing.Pool(processes=4)#建立进程池,默认为cpu核数 print(href) print(&apos;区域数量为&apos;+str(len(href))) for i in range(len(href)): for index in range(1,3):#取1到5面的内容 if index==1: realurl=href[i] else: realurl=href[i]+&apos;p&apos;+str(index)#注意到每面的链接就是在第一面的基础上加上p+页数 print(realurl) result=pool.apply_async(getValue,(realurl,),callback=conbineResult)#添加进程，并且调用回调函数conbineResult处理结果 pool.close()#关闭进程池，不再添加进程 pool.join()#因为是非阻塞的，主进程继续执行直到遇到.join()等待pool中所有子进程完毕，主进程才继续 if result.successful(): print(&apos;successful&apos;) &apos;&apos;&apos; with open(r&apos;D:/outp.txt&apos;,&apos;rb&apos;) as f:#对于多次dump,pickle每次序列化生成的字符串有独立头尾,pickle.load()只会读取一个完整的结果,需要循环读出 try: while True: datas=pickle.load(f)#取出数据 print(datas) except: print(&apos;success&apos;) &apos;&apos;&apos;if __name__ == &quot;__main__&quot;: main() 参考文献 Python爬虫利器二之Beautiful Soup的用法 | 静觅 python多进程的理解 multiprocessing Process join run – 李皮筋 – 博客园 使用python+selenium+Firefox登录大众点评 – wmq104的博客 – CSDN博客 Python爬虫模拟登录的黑魔法 – 简书]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之浅谈万物皆对象(1)]]></title>
    <url>%2F2019%2F02%2F26%2FJava%E4%B9%8B%E6%B5%85%E8%B0%88%E4%B8%87%E7%89%A9%E7%9A%86%E5%AF%B9%E8%B1%A1-1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;“如果我们说另一种不同的语言，那么我们就会发觉一个有些不同的世界。”&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;——Luduing Wittgerstein（1889-1951） &emsp;&emsp;尽管Java是基于C++的，但是相比之下，Java是一种更“纯粹”的面向对象程序设计语言（OOP），即“万物皆对象”。 面向过程和面向对象的区别&emsp;&emsp;面向过程和面向对象是编程中两个非常基本的思想。很多人之前学过的C语言就是面向过程的语言，而C++、Java等就是面向对象的语言。目前显然面向对象是一种主流的语言，工作和实习中也以面向对象为主。 &emsp;&emsp;首先，面向过程和面向对象都是软件分析、设计和开发的一种思想，它指导着人们以不同的方式去分析、设计和开发软件。但这两者并不对立，而是相辅相成的关系，我们去写一个软件，既可以用到面向对象的思想，也可以用到面向过程的思想。 &emsp;&emsp;那么什么叫面向过程思想？当你碰到一些简单的问题时，我们会自然而然地使用面向过程的思想。比如说你碰到一件事，你会想你第一步应该怎么做，第二步应该怎么做，第三步应该怎么做……当你这么想的时候，你就自然而然地使用了面向过程的思想。举一个简单的例子，如何开车？很多人去学车，学车就会面对一个问题，一辆车怎么发动起来，就会考虑：第一步点火，第二步踩离合，第三步踩油门，这个过程就是面向过程。 &emsp;&emsp;面向过程适合简单、不需要协作的事务。就比如之前提到的开车，开车不需要协助，你自己一个人就够了，这就叫简单的事务。但是当我们去思考比较复杂的问题，比如如何造车？这个时候你就会发现需要很多人去协作，很难明确列出几个步骤。这个时候就需要用面向对象的思想。现在问你如何造车，一个正常人都不会去思考我第一步造什么，第二步造什么，而是首先去思考这车都由哪些物件（Object）组成，用一种设计的眼光去看问题。车是由车窗、轮胎、发动机等部件组成，思考如何造车，我们就会去思考怎么造这些部件。 &emsp;&emsp;由此可见，面对复杂的问题，我们会自然而然地去使用面向对象的思想，所以面向对象的思想适合开发一些大型的软件。 &emsp;&emsp;面向对象（Object）思想更契合人的思维模式。那么回到之前的造车问题，我们会发现，造轮胎、造发动机也是有步骤的，也就是说，面向对象和面向过程是相辅相成的。面向对象在宏观上把握整体的软件设计，微观上仍然需要使用面向过程。非常典型的问题就是，我们编程永远离不开方法，而方法就是典型的面向过程的产物。 面向对象和面向过程的总结： 都是解决问题的思维方式，都是代码组织的方式 解决简单问题可以使用面向过程 解决复杂问题：宏观上使用面向对象把握，微观处理上仍然是面向过程]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总得写点什么]]></title>
    <url>%2F2019%2F01%2F17%2F%E6%80%BB%E5%BE%97%E5%86%99%E4%BA%9B%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[其实，我根本没尽全力做过一件事&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;好像每一个拥有技术博客的人，都会在开篇写一写为什么自己要建一个技术博客。就好比程序员在学一门新的语言，第一段代码总要写一个Hello World，以表示对这个世界的尊重。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么我为什么要建这样一个Blog呢？大家都这样做，那我也跟个潮流？或者把自己掌握的知识写出来向小伙伴们炫耀？都不是。我只是想尽全力做一件事，从写Blog开始。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去七千多个日日夜夜幻灯片似的在我的眼前滑过： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;早上8点半，闹钟响了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我磨蹭了20分钟，不情不愿地爬起了床。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我决定，今天一定要看完半本专业书，把简历好好润色一下，再写两篇心得。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我坐在书桌前，翻了10分钟的书后，就掏出了手机。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我想，没关系，我就刷一下朋友圈。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果，这一天，我刷了一小时的朋友圈，看了三小时的剧，翻了两个多小时的微博，打了半天的王者农药。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看一眼时间，怎么又要到零点了？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我觉得很懊恼，开始自责，犹豫要不要熬夜把这些事做了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过一会，我想想算了，晚上效率也不高，还是明天再继续吧。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是，我躺到床上，开始玩起了手机。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“别难过，你已经尽力啦。” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做一些事失败后，总会收到这样的安慰。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;听到这句话，觉得宽慰的同时，总会有点心虚： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我真的尽力了吗？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，仔细回想，那些我说自己尽力了的时候，好像都不算尽力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;想考研能考上个好学校，知道微积分是弱项，但也只是嘴上说着要好好复习微积分，带回来的课本还躺在行李箱里蒙尘。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;希望拥有一份自己喜欢的工作，也只是在daydreaming时幻想一下在BAT工作时的场景，根本没有认真地去规划和实习。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;喜欢上一个女孩，因为怕被拒绝，所以只敢有事没事点个赞，发句不痛不痒的晚安，却从不去真的表白。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只是，为了让自己从悔恨的苦海里挣脱出来，也就顺势接受了这样的宽慰。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但在心里那个不为人知的回放室里，每每想到那些虚度的时光，总会有些羞愧，又会莫名感到松了口气： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你看，这些其实都是因为我没尽力，才做不到的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在尽力这件事上，我们总是异常地谨慎。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于很多人来说，相比于失败或努力后失败，更可怕的可能是———— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用尽全力去做一件事，然后失败了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就像是在昭示着： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你没天赋的，你不行的，你根本就不是做这块的料。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种挫败感，远远胜过于没怎么尽力而失败的挫败感，并且总会让人觉得，当初用尽全力的自己，仿佛就像是个二傻子。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是，大部分时候，我们只是象征性地努力一下，然后就躺平认输了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但嘴上还是会喃喃着，安抚自己和身边的人： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;啊，没事，我已经尽力了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;几个月前，看了一部很暖也很热血的日剧，名字叫《重版出来》，讲的是一群漫画社的成员为了让漫画“再次印刷”的故事。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主角黑泽是一个元气满满的少女，因为喜欢漫画选择去做了一个漫画编辑，做每一件事都会用尽全力，不管它看上去多不可实现、无趣枯燥。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与之完全相反的是销售部的小泉君。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;他想成为一个编辑，却意外被分到销售部，每天都陷入丧气和绝望之中，销售额时常垫底，因为存在感太低，被称作为“幽灵”。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;转折点在一次推广的任务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;黑泽被派去帮忙小泉君，销售一本毫无名气、销量惨淡的漫画书。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;黑泽主动与书店店员说明这本书的创作初衷和现实情况，希望作者能在这家书店开签售会。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了增强漫画的曝光率，黑泽还向店员提出，在相关的区域设置漫画专栏，以增加二者的销售额。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果，这本漫画深受读者的喜欢，卖得很火，真的“重版出来”了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;小泉君被黑泽的干劲所感染，叹道： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看到这的时候，虽然也并非多新颖的励志桥段，但还是被戳中了泪点。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;仔细想想，大概是因为我和小泉君一样，从未体会过“用尽全力”是一种怎样的感受。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们总会去思考———— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这件事成功的概率会有多少？它值得投入这么多吗？如果尽力后，还失败，该怎么办？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;而在这些日剧里，那些我们不敢有的“尽力生活”全都被展现了出来。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在看到主角不顾一切往前冲的背影时，才恍然发现： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原来，用尽全力、不计后果地去尝试，一点也不傻啊。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，这世上很少有人能说： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一生，我用尽全力去活了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大部分人都只是畏畏缩缩、拖拖拉拉地过完了大半辈子，然后在快离世的时候，才遗憾感叹，一辈子竟然就这样结束了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;日本作家中岛敦在《山月记》里写过一段很扎心的话： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因为害怕自己并非明珠而不敢刻苦琢磨，又因为有几分相信自己是明珠，而不能与瓦砾碌碌为伍…… &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如今想起来，我真是空费了自己那一点仅有的才能，徒然在口头上卖弄着什么“人生一事不为则太长，欲为一事则太短”的警句，可事实是，唯恐暴露才华不足的卑怯的畏惧，和厌恶钻研刻苦的惰怠，就是我的全部了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实上，在出壳之前，谁也不知道自己是明珠还是瓦砾。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可是，是明珠还是瓦砾，又有这么重要吗？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关键是，我们这只有一次的人生———— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本来就应该，用尽全力、半点不浪费地去活啊。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F01%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
